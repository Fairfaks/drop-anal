import asyncio
import aiohttp
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass, field
import json
import re
from urllib.parse import urlparse
import whois
import dns.resolver
from collections import defaultdict
import numpy as np
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

import re

def load_spam_pattern(filepath: str) -> re.Pattern:
    words = []
    with open(filepath, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            words.append(re.escape(line))
    words.sort(key=len, reverse=True)
    pattern = r'(?i)(?:^|\s)(' + '|'.join(words) + r')(?:\s|$)'
    return re.compile(pattern)


# –≥–¥–µ-—Ç–æ –≤ –Ω–∞—á–∞–ª–µ —Å–∫—Ä–∏–ø—Ç–∞
spam_pattern = load_spam_pattern('spam_words.txt')

@dataclass
class SEOMetrics:
    """SEO –º–µ—Ç—Ä–∏–∫–∏ –¥–æ–º–µ–Ω–∞"""
    domain_age: Optional[int] = None  # –í –¥–Ω—è—Ö
    registration_date: Optional[datetime] = None
    expiry_date: Optional[datetime] = None
    registrar: Optional[str] = None

    # Wayback –º–µ—Ç—Ä–∏–∫–∏
    first_snapshot: Optional[datetime] = None
    last_snapshot: Optional[datetime] = None
    total_snapshots: int = 0
    snapshot_frequency: float = 0.0  # –°–Ω–∏–º–∫–æ–≤ –≤ –º–µ—Å—è—Ü

    # –ò—Å—Ç–æ—Ä–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_changes: int = 0
    language_changes: List[str] = field(default_factory=list)
    niche_changes: List[str] = field(default_factory=list)

    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    cms_history: List[str] = field(default_factory=list)
    server_changes: List[str] = field(default_factory=list)
    ssl_history: List[bool] = field(default_factory=list)

    # –°–ø–∞–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ
    spam_periods: List[Tuple[datetime, datetime]] = field(default_factory=list)
    clean_periods: List[Tuple[datetime, datetime]] = field(default_factory=list)
    spam_ratio: float = 0.0

    # –°—Å—ã–ª–æ—á–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å (–∏–∑ Wayback)
    outbound_links_history: Dict[str, int] = field(default_factory=dict)
    internal_links_avg: float = 0.0
    external_links_avg: float = 0.0

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
    was_hacked: bool = False
    had_malware: bool = False
    was_penalized: bool = False  # –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    suitable_for_pbn: bool = True
    risk_score: float = 0.0  # 0-100, –≥–¥–µ 0 - –±–µ–∑–æ–ø–∞—Å–Ω–æ, 100 - –æ–ø–∞—Å–Ω–æ

    # –ù–æ–≤—ã–µ –ø–æ–ª—è –∏–∑ Backorder API
    backorder_price: Optional[float] = None  # price
    backorder_age_years: Optional[int] = None  # old
    backorder_rkn_block: Optional[bool] = None  # rkn
    backorder_judicial: Optional[bool] = None  # judicial
    backorder_links_count: Optional[int] = None  # links



@dataclass
class ContentAnalysis:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–º–µ–Ω–∞"""
    primary_language: str = "unknown"
    languages_found: Set[str] = field(default_factory=set)
    primary_niche: str = "unknown"
    niches_found: Set[str] = field(default_factory=set)

    # –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    avg_content_length: float = 0.0
    has_quality_content: bool = False
    content_uniqueness_score: float = 0.0

    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞
    page_types: Dict[str, int] = field(default_factory=dict)  # blog, shop, corporate, etc
    has_blog: bool = False
    has_shop: bool = False

    # –ú–µ–¥–∏–∞
    images_count_avg: float = 0.0
    videos_found: bool = False

    # SEO —ç–ª–µ–º–µ–Ω—Ç—ã
    has_meta_descriptions: bool = False
    has_proper_headings: bool = False
    has_schema_markup: bool = False


class PBNDomainAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è PBN"""

    def __init__(self):
        self.session = None
        self.spam_patterns = self._compile_spam_patterns()
        self.quality_patterns = self._compile_quality_patterns()
        self.cms_patterns = self._compile_cms_patterns()
        self.niche_keywords = self._load_niche_keywords()

    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def _fetch_backorder_data(self, domain: str, result: Dict):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ü–µ–Ω—É, –≤–æ–∑—Ä–∞—Å—Ç, —Å—Å—ã–ª–∫–∏, —Ñ–ª–∞–≥–∏ –†–ö–ù/—Å—É–¥ –∏–∑ Backorder.ru API"""
        api_url = (
            f"https://backorder.ru/json/"
            f"?order=desc&domainname={domain}"
            f"&view_all=1&by=hotness&page=1&items=1"
        )
        try:
            async with self.session.get(api_url) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    if isinstance(data, list) and data:
                        entry = data[0]
                        metrics = result['metrics']
                        metrics.backorder_price        = float(entry.get('price', 0))
                        metrics.backorder_age_years    = int(entry.get('old', 0))
                        metrics.backorder_links_count  = int(entry.get('links', 0))
                        metrics.backorder_rkn_block    = bool(entry.get('rkn', False))
                        metrics.backorder_judicial     = bool(entry.get('judicial', False))
        except Exception as e:
            logger.warning(f"Backorder API error for {domain}: {e}")

    def _compile_spam_patterns(self):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–∞–º–∞"""
        spam_indicators = [
            # –Ø–≤–Ω—ã–π —Å–ø–∞–º
            r'casino|–∫–∞–∑–∏–Ω–æ|porn|–ø–æ—Ä–Ω–æ|xxx|sex|—Å–µ–∫—Å',
            r'viagra|–≤–∏–∞–≥—Ä–∞|cialis|—Å–∏–∞–ª–∏—Å',
            r'gambling|betting|—Å—Ç–∞–≤–∫–∏|–±—É–∫–º–µ–∫–µ—Ä',
            r'crypto|bitcoin|–±–∏—Ç–∫–æ–∏–Ω|trading',

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤–∑–ª–æ–º–∞
            r'hacked by|defaced by|0wned by',
            r'<script>.*?eval\(|base64_decode|document\.write',
            r'malware|virus|trojan',

            # –ö–∏—Ç–∞–π—Å–∫–∏–π —Å–ø–∞–º
            r'[\u4e00-\u9fff]{50,}',  # –ú–Ω–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–¥—Ä—è–¥

            # –§–∞—Ä–º–∞ —Å–ø–∞–º
            r'pharmacy|–∞–ø—Ç–µ–∫–∞|pills|—Ç–∞–±–ª–µ—Ç–∫–∏|medications'
        ]
        return [re.compile(pattern, re.IGNORECASE | re.DOTALL) for pattern in spam_indicators]

    def _compile_quality_patterns(self):
        """–ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return {
            'blog': re.compile(r'blog|article|post|–∞–≤—Ç–æ—Ä|–æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ|—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', re.I),
            'corporate': re.compile(r'about us|–æ –Ω–∞—Å|company|–∫–æ–º–ø–∞–Ω–∏—è|services|—É—Å–ª—É–≥–∏', re.I),
            'shop': re.compile(r'shop|–º–∞–≥–∞–∑–∏–Ω|cart|–∫–æ—Ä–∑–∏–Ω–∞|buy|–∫—É–ø–∏—Ç—å|price|—Ü–µ–Ω–∞', re.I),
            'news': re.compile(r'news|–Ω–æ–≤–æ—Å—Ç–∏|press|–ø—Ä–µ—Å—Å|latest|–ø–æ—Å–ª–µ–¥–Ω–∏–µ', re.I),
            'education': re.compile(r'learn|–æ–±—É—á–µ–Ω–∏–µ|course|–∫—É—Ä—Å|tutorial|—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', re.I)
        }

    def _compile_cms_patterns(self):
        """–ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è CMS"""
        return {
            'wordpress': re.compile(r'wp-content|wp-includes|wordpress', re.I),
            'joomla': re.compile(r'joomla|/components/|/modules/', re.I),
            'drupal': re.compile(r'drupal|/sites/default/', re.I),
            'bitrix': re.compile(r'bitrix|/bitrix/', re.I),
            'modx': re.compile(r'modx|/assets/components/', re.I),
            'shopify': re.compile(r'shopify|myshopify\.com', re.I),
            'wix': re.compile(r'wix\.com|wixsite', re.I),
            'tilda': re.compile(r'tilda\.(cc|ws)', re.I),
            'django': re.compile(r'django|csrfmiddlewaretoken', re.I),
            'laravel': re.compile(r'laravel|/public/index\.php', re.I)
        }

    def _load_niche_keywords(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∏—à"""
        return {
            'finance': ['–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∑–∞–π–º', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '—Ñ–∏–Ω–∞–Ω—Å—ã', 'bank', 'loan', 'investment'],
            'realestate': ['–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '–∫–≤–∞—Ä—Ç–∏—Ä–∞', '–¥–æ–º', '–∞—Ä–µ–Ω–¥–∞', 'real estate', 'apartment', 'rent'],
            'health': ['–∑–¥–æ—Ä–æ–≤—å–µ', '–º–µ–¥–∏—Ü–∏–Ω–∞', '–≤—Ä–∞—á', '–∫–ª–∏–Ω–∏–∫–∞', 'health', 'medical', 'doctor', 'clinic'],
            'tech': ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 'it', 'software', '–∫–æ–¥', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'technology'],
            'education': ['–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å—ã', '—à–∫–æ–ª–∞', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 'education', 'school'],
            'travel': ['–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', '—Ç—É—Ä–∏–∑–º', '–æ—Ç–µ–ª—å', '—Ç—É—Ä', 'travel', 'tourism', 'hotel', 'vacation'],
            'food': ['–µ–¥–∞', '—Ä–µ—Ü–µ–ø—Ç', '–∫—É–ª–∏–Ω–∞—Ä–∏—è', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', 'food', 'recipe', 'cooking', 'restaurant'],
            'fashion': ['–º–æ–¥–∞', '–æ–¥–µ–∂–¥–∞', '—Å—Ç–∏–ª—å', 'fashion', 'clothes', 'style', 'outfit'],
            'auto': ['–∞–≤—Ç–æ', '–º–∞—à–∏–Ω–∞', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'car', 'vehicle', 'automotive'],
            'sport': ['—Å–ø–æ—Ä—Ç', '—Ñ–∏—Ç–Ω–µ—Å', '—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞', 'sport', 'fitness', 'workout', 'gym']
        }

    async def analyze_domain(self, domain: str) -> Dict:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞ –¥–ª—è PBN"""
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞: {domain}")

        result = {
            'domain': domain,
            'metrics': SEOMetrics(),
            'content': ContentAnalysis(),
            'wayback_data': {},
            'recommendations': [],
            'warnings': [],
            'pbn_score': 0.0
        }

        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º WHOIS
        await self._check_whois(domain, result)

        # 1.5. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ü–µ–Ω–µ –∏ –≤–æ–∑—Ä–∞—Å—Ç—É –∏–∑ Backorder.ru
        await self._fetch_backorder_data(domain, result)

        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ Wayback Machine
        await self._analyze_wayback_history(domain, result)
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º DNS –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã
        await self._check_dns_records(domain, result)

        # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º PBN Score
        self._calculate_pbn_score(result)

        # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        self._generate_recommendations(result)

        return result

    async def _check_whois(self, domain: str, result: Dict):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
        try:
            w = whois.whois(domain)

            if w.creation_date:
                creation = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date
                result['metrics'].registration_date = creation
                result['metrics'].domain_age = (datetime.now() - creation).days

            if w.expiration_date:
                expiry = w.expiration_date[0] if isinstance(w.expiration_date, list) else w.expiration_date
                result['metrics'].expiry_date = expiry

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏—Å—Ç–µ–∫–∞–µ—Ç –ª–∏ –¥–æ–º–µ–Ω —Å–∫–æ—Ä–æ
                days_until_expiry = (expiry - datetime.now()).days
                if days_until_expiry < 30:
                    result['warnings'].append(f"–î–æ–º–µ–Ω –∏—Å—Ç–µ–∫–∞–µ—Ç —á–µ—Ä–µ–∑ {days_until_expiry} –¥–Ω–µ–π!")

            result['metrics'].registrar = w.registrar

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ WHOIS –¥–ª—è {domain}: {e}")
            result['warnings'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å WHOIS –¥–∞–Ω–Ω—ã–µ")

    async def _check_dns_records(self, domain: str, result: Dict):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç DNS –∑–∞–ø–∏—Å–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º A –∑–∞–ø–∏—Å–∏
            answers = dns.resolver.resolve(domain, 'A')
            result['wayback_data']['current_ips'] = [str(rdata) for rdata in answers]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º MX –∑–∞–ø–∏—Å–∏ (–Ω–∞–ª–∏—á–∏–µ –ø–æ—á—Ç—ã)
            try:
                mx_answers = dns.resolver.resolve(domain, 'MX')
                result['wayback_data']['has_mx'] = len(mx_answers) > 0
            except:
                result['wayback_data']['has_mx'] = False

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ DNS –¥–ª—è {domain}: {e}")
            result['warnings'].append("–î–æ–º–µ–Ω –Ω–µ —Ä–µ–∑–æ–ª–≤–∏—Ç—Å—è –∏–ª–∏ DNS –ø—Ä–æ–±–ª–µ–º—ã")
            result['metrics'].suitable_for_pbn = False

    async def _analyze_wayback_history(self, domain: str, result: Dict):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –≤ Wayback Machine"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–Ω–∏–º–∫–∏
            api_url = f"https://web.archive.org/cdx/search/cdx?url={domain}&output=json&fl=timestamp,original,mimetype,statuscode,digest"

            async with self.session.get(api_url) as response:
                if response.status != 200:
                    return

                data = await response.json()
                if len(data) <= 1:
                    result['warnings'].append("–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –≤ Wayback Machine")
                    return

                records = data[1:]
                result['metrics'].total_snapshots = len(records)

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                timestamps = [datetime.strptime(r[0], "%Y%m%d%H%M%S") for r in records]
                result['metrics'].first_snapshot = min(timestamps)
                result['metrics'].last_snapshot = max(timestamps)

                # –ß–∞—Å—Ç–æ—Ç–∞ —Å–Ω–∏–º–∫–æ–≤
                if len(timestamps) > 1:
                    total_months = (max(timestamps) - min(timestamps)).days / 30
                    result['metrics'].snapshot_frequency = len(timestamps) / total_months if total_months > 0 else 0

                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–Ω–∏–º–∫–∏ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                await self._analyze_content_periods(domain, records, result)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ Wayback –¥–ª—è {domain}: {e}")

    async def _analyze_content_periods(self, domain: str, records: List, result: Dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º"""
        # –ë–µ—Ä—ë–º —Å–Ω–∏–º–∫–∏ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        snapshots_to_analyze = self._select_representative_snapshots(records, max_snapshots=20)

        content_history = []
        spam_periods = []
        clean_periods = []
        current_period_start = None
        current_period_is_spam = False
        current_period_spam_keywords = set()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        result['content_history_detailed'] = []

        for i, record in enumerate(snapshots_to_analyze):
            timestamp = record[0]
            url = f"https://web.archive.org/web/{timestamp}/{record[1]}"

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–Ω–∏–º–æ–∫
            analysis = await self._analyze_snapshot_content(url, domain)
            content_history.append({
                'timestamp': datetime.strptime(timestamp, "%Y%m%d%H%M%S"),
                'analysis': analysis
            })

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
            result['content_history_detailed'].append({
                'timestamp': datetime.strptime(timestamp, "%Y%m%d%H%M%S"),
                'url': url,
                'is_spam': analysis.get('is_spam', False),
                'spam_keywords': analysis.get('spam_keywords', []),
                'language': analysis.get('language', 'unknown'),
                'niche': analysis.get('niche', 'unknown'),
                'cms': analysis.get('cms', 'unknown'),
                'content_length': analysis.get('content_length', 0)
            })

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥—ã —Å–ø–∞–º–∞/—á–∏—Å—Ç–æ—Ç—ã
            is_spam = analysis.get('is_spam', False)

            if i == 0:
                current_period_start = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
                current_period_is_spam = is_spam
                if is_spam:
                    current_period_spam_keywords.update(analysis.get('spam_keywords', []))
            elif is_spam != current_period_is_spam:
                # –ü–µ—Ä–∏–æ–¥ –∏–∑–º–µ–Ω–∏–ª—Å—è
                period_end = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
                if current_period_is_spam:
                    spam_periods.append((current_period_start, period_end, list(current_period_spam_keywords)))
                else:
                    clean_periods.append((current_period_start, period_end))

                current_period_start = period_end
                current_period_is_spam = is_spam
                current_period_spam_keywords = set(analysis.get('spam_keywords', [])) if is_spam else set()
            elif is_spam:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–ø–∞–º-—Å–ª–æ–≤–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –ø–µ—Ä–∏–æ–¥—É
                current_period_spam_keywords.update(analysis.get('spam_keywords', []))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥
        if current_period_start and len(snapshots_to_analyze) > 0:
            last_timestamp = datetime.strptime(snapshots_to_analyze[-1][0], "%Y%m%d%H%M%S")
            if current_period_is_spam:
                spam_periods.append((current_period_start, last_timestamp, list(current_period_spam_keywords)))
            else:
                clean_periods.append((current_period_start, last_timestamp))

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ (—Ç–µ–ø–µ—Ä—å spam_periods —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ—Ä—Ç–µ–∂–∏ –∏–∑ 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        result['metrics'].spam_periods = [(start, end) for start, end, _ in spam_periods]
        result['metrics'].clean_periods = clean_periods

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–∞–º-–ø–µ—Ä–∏–æ–¥–∞—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        result['spam_periods_with_keywords'] = spam_periods

        # –°—á–∏—Ç–∞–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ø–∞–º–∞
        total_days = (result['metrics'].last_snapshot - result['metrics'].first_snapshot).days
        spam_days = sum((end - start).days for start, end, _ in spam_periods)
        result['metrics'].spam_ratio = spam_days / total_days if total_days > 0 else 0

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self._analyze_content_changes(content_history, result)


    async def _analyze_snapshot_content(self, url: str, domain: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–¥–Ω–æ–≥–æ —Å–Ω–∏–º–∫–∞ –∏–∑ Wayback Machine:
        - –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É
        - –ü–∞—Ä—Å–∏—Ç HTML –≤ BeautifulSoup
        - –î–æ—Å—Ç–∞—ë—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
        - –ò—â–µ—Ç —Å–ø–∞–º-—Å–ª–æ–≤–∞
        - –°—á–∏—Ç–∞–µ—Ç –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –∏ —Å—Å—ã–ª–∫–∏
        """
        analysis = {
            'is_spam': False,
            'spam_keywords': [],
            'language': 'unknown',
            'niche': 'unknown',
            'cms': 'unknown',
            'content_length': 0,
            'links_internal': 0,
            'links_external': 0,
            'has_quality_signals': False
        }

        try:
            # 1) –°–∫–∞—á–∏–≤–∞–µ–º
            async with self.session.get(url) as response:
                if response.status != 200:
                    return analysis

                # 2) –ü–æ–ª—É—á–∞–µ–º HTML
                html = await response.text()

            # 3) –ü–∞—Ä—Å–∏–º –µ–≥–æ
            soup = BeautifulSoup(html, 'lxml')

            # 4) –î–æ—Å—Ç–∞—ë–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
            text = soup.get_text(separator=' ', strip=True)
            analysis['content_length'] = len(text)

            # 5) –ò—â–µ–º —Å–ø–∞–º-—Å–ª–æ–≤–∞
            found = {m.group(1).strip() for m in spam_pattern.finditer(text)}
            if found:
                analysis['is_spam'] = True
                analysis['spam_keywords'] = list(found)

            # 6) –ü—Ä–∏–º–µ—Ä: —Å—á–∏—Ç–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            for a in soup.find_all('a', href=True):
                href = a['href'].lower()
                if href.startswith('/') or domain in href:
                    analysis['links_internal'] += 1
                else:
                    analysis['links_external'] += 1

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                if re.search(r'[–∞-—è–ê-–Ø]{10,}', text):
                    analysis['language'] = 'ru'
                elif re.search(r'[a-zA-Z]{10,}', text):
                    analysis['language'] = 'en'

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∏—à—É
                text_lower = text.lower()
                for niche, keywords in self.niche_keywords.items():
                    if any(keyword in text_lower for keyword in keywords):
                        analysis['niche'] = niche
                        break

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º CMS
                html_lower = html.lower()
                for cms, pattern in self.cms_patterns.items():
                    if pattern.search(html_lower):
                        analysis['cms'] = cms
                        break

                # –°—á–∏—Ç–∞–µ–º —Å—Å—ã–ª–∫–∏
                links = soup.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    if href.startswith('http'):
                        if domain in href:
                            analysis['links_internal'] += 1
                        else:
                            analysis['links_external'] += 1
                    else:
                        analysis['links_internal'] += 1

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
                if (soup.find('article') or soup.find('main') or
                        len(soup.find_all(['h1', 'h2', 'h3'])) > 3):
                    analysis['has_quality_signals'] = True



        except Exception as e:

            logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–Ω–∏–º–∫–∞ {url}: {e}")

        return analysis

    def _select_representative_snapshots(self, records: List, max_snapshots: int = 20) -> List:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ —Å–Ω–∏–º–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        if len(records) <= max_snapshots:
            return records

        # –ë–µ—Ä—ë–º —Å–Ω–∏–º–∫–∏ —Å —Ä–∞–≤–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
        step = len(records) // max_snapshots
        selected = []

        for i in range(0, len(records), step):
            selected.append(records[i])

        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∞–µ–º –ø–µ—Ä–≤—ã–π –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π
        if records[0] not in selected:
            selected.insert(0, records[0])
        if records[-1] not in selected:
            selected.append(records[-1])

        return selected[:max_snapshots]

    def _analyze_content_changes(self, content_history: List[Dict], result: Dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not content_history:
            return

        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        languages = set()
        niches = set()
        cms_list = []

        for item in content_history:
            analysis = item['analysis']
            if analysis['language'] != 'unknown':
                languages.add(analysis['language'])
            if analysis['niche'] != 'unknown':
                niches.add(analysis['niche'])
            if analysis['cms'] != 'unknown':
                cms_list.append(analysis['cms'])

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        result['content'].languages_found = languages
        result['content'].primary_language = max(languages, key=lambda x: sum(
            1 for i in content_history if i['analysis']['language'] == x)) if languages else 'unknown'

        result['content'].niches_found = niches
        result['content'].primary_niche = max(niches, key=lambda x: sum(
            1 for i in content_history if i['analysis']['niche'] == x)) if niches else 'unknown'

        result['metrics'].cms_history = list(set(cms_list))

        # –°—á–∏—Ç–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        prev_lang = None
        prev_niche = None

        for item in content_history:
            analysis = item['analysis']

            if prev_lang and prev_lang != analysis['language'] and analysis['language'] != 'unknown':
                result['metrics'].language_changes.append(f"{prev_lang} -> {analysis['language']}")
            prev_lang = analysis['language']

            if prev_niche and prev_niche != analysis['niche'] and analysis['niche'] != 'unknown':
                result['metrics'].niche_changes.append(f"{prev_niche} -> {analysis['niche']}")
            prev_niche = analysis['niche']

        # –°—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        result['content'].avg_content_length = np.mean([i['analysis']['content_length'] for i in content_history])
        result['metrics'].internal_links_avg = np.mean([i['analysis']['links_internal'] for i in content_history])
        result['metrics'].external_links_avg = np.mean([i['analysis']['links_external'] for i in content_history])

    def _calculate_pbn_score(self, result: Dict):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç PBN Score"""
        score = 100.0

        metrics = result['metrics']
        content = result['content']

        # –í–æ–∑—Ä–∞—Å—Ç –¥–æ–º–µ–Ω–∞ (—á–µ–º —Å—Ç–∞—Ä—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        if metrics.domain_age:
            if metrics.domain_age < 365:  # –ú–µ–Ω—å—à–µ –≥–æ–¥–∞
                score -= 20
            elif metrics.domain_age < 730:  # –ú–µ–Ω—å—à–µ 2 –ª–µ—Ç
                score -= 10
        else:
            score -= 15

        # –ò—Å—Ç–æ—Ä–∏—è –≤ Wayback
        if metrics.total_snapshots < 10:
            score -= 15
        elif metrics.total_snapshots < 50:
            score -= 5

        # –°–ø–∞–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        if metrics.spam_ratio > 0.5:  # –ë–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–∏ –±—ã–ª —Å–ø–∞–º–æ–º
            score -= 40
        elif metrics.spam_ratio > 0.2:
            score -= 20
        elif metrics.spam_ratio > 0.1:
            score -= 10

        # –ò–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∏—à–∏
        if len(metrics.niche_changes) > 2:
            score -= 15
        elif len(metrics.niche_changes) > 1:
            score -= 5

        # –ò–∑–º–µ–Ω–µ–Ω–∏—è —è–∑—ã–∫–∞
        if len(metrics.language_changes) > 1:
            score -= 10

        # –ß–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        if metrics.snapshot_frequency < 0.5:  # –ú–µ–Ω—å—à–µ 1 —Å–Ω–∏–º–∫–∞ –≤ 2 –º–µ—Å—è—Ü–∞
            score -= 10

        # –ü–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        if metrics.last_snapshot:
            days_since_last = (datetime.now() - metrics.last_snapshot).days
            if days_since_last > 365:  # –ë–æ–ª—å—à–µ –≥–æ–¥–∞ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª—Å—è
                score -= 15
            elif days_since_last > 180:
                score -= 5

        # –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if content.avg_content_length < 500:
            score -= 10

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if not result['wayback_data'].get('current_ips'):
            score -= 30  # –î–æ–º–µ–Ω –Ω–µ —Ä–µ–∑–æ–ª–≤–∏—Ç—Å—è

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—á—ë—Ç
        result['pbn_score'] = max(0, score)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å
        result['metrics'].suitable_for_pbn = score >= 60
        result['metrics'].risk_score = 100 - score

    def _generate_recommendations(self, result: Dict):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        score = result['pbn_score']
        metrics = result['metrics']

        if score >= 80:
            result['recommendations'].append("‚úÖ –û—Ç–ª–∏—á–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª—è PBN")
            result['recommendations'].append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø–æ–∫—É–ø–∫–µ")
        elif score >= 60:
            result['recommendations'].append("‚ö†Ô∏è –•–æ—Ä–æ—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ —Ä–∏—Å–∫–∞–º–∏")
            result['recommendations'].append("–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
        else:
            result['recommendations'].append("‚ùå –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –¥–ª—è PBN")
            result['recommendations'].append("–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø–æ–∫—É–ø–∫–µ")

        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if metrics.spam_ratio > 0.1:
            result['recommendations'].append(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–µ—Ä–∏–æ–¥—ã —Å–ø–∞–º–∞ ({metrics.spam_ratio * 100:.1f}% –≤—Ä–µ–º–µ–Ω–∏)")
            result['recommendations'].append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á–µ—Ä–µ–∑ Ahrefs/Majestic –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–ª–æ—Ö–∏—Ö —Å—Å—ã–ª–æ–∫")

        if len(metrics.niche_changes) > 0:
            result['recommendations'].append(f"üìä –î–æ–º–µ–Ω –º–µ–Ω—è–ª —Ç–µ–º–∞—Ç–∏–∫—É {len(metrics.niche_changes)} —Ä–∞–∑")
            result['recommendations'].append("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ç–µ–∫—É—â–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–∏–º —Ü–µ–ª—è–º")

        if metrics.domain_age and metrics.domain_age > 1825:  # 5 –ª–µ—Ç
            result['recommendations'].append("üëç –í–æ–∑—Ä–∞—Å—Ç –¥–æ–º–µ–Ω–∞ > 5 –ª–µ—Ç - —Ö–æ—Ä–æ—à–∏–π —Ç—Ä–∞—Å—Ç")

        if metrics.last_snapshot:
            days_inactive = (datetime.now() - metrics.last_snapshot).days
            if days_inactive > 180:
                result['recommendations'].append(f"‚è∞ –î–æ–º–µ–Ω –Ω–µ–∞–∫—Ç–∏–≤–µ–Ω {days_inactive} –¥–Ω–µ–π")
                result['recommendations'].append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –≤ Google")

        if len(metrics.cms_history) > 0:
            result['recommendations'].append(f"üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å CMS: {', '.join(metrics.cms_history)}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è PBN
        if metrics.external_links_avg > 20:
            result['recommendations'].append(
                "üîó –ú–Ω–æ–≥–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ - –≤–æ–∑–º–æ–∂–Ω–æ, —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ø—Ä–æ–¥–∞–∂–∏ —Å—Å—ã–ª–æ–∫")

        if not result['wayback_data'].get('has_mx', True):
            result['recommendations'].append("üìß –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç MX –∑–∞–ø–∏—Å–∏ - –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ—á—Ç—É –ø–æ—Å–ª–µ –ø–æ–∫—É–ø–∫–∏")
# –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∞–º-–ø–∞—Ç—Ç–µ—Ä–Ω –æ–¥–∏–Ω —Ä–∞–∑
        self.spam_pattern = load_spam_pattern('spam_words.txt')
        # –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω self.spam_patterns

class PBNBulkAnalyzer:
    """–ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è PBN"""

    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def analyze_domains(self, domains: List[str], output_file: str = "pbn_analysis.xlsx"):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤"""
        results = []

        async with PBNDomainAnalyzer() as analyzer:
            tasks = []

            for domain in domains:
                task = self._analyze_with_semaphore(analyzer, domain)
                tasks.append(task)

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            for i, task in enumerate(asyncio.as_completed(tasks)):
                result = await task
                results.append(result)
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(domains)} –¥–æ–º–µ–Ω–æ–≤")

        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._export_results(results, output_file)

        return results

    async def _analyze_with_semaphore(self, analyzer: PBNDomainAnalyzer, domain: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–º–µ–Ω —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏"""
        async with self.semaphore:
            try:
                return await analyzer.analyze_domain(domain)
            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {domain}: {e}")
                return {
                    'domain': domain,
                    'error': str(e),
                    'pbn_score': 0,
                    'metrics': SEOMetrics(),
                    'content': ContentAnalysis()
                }

    def _export_results(self, results: List[Dict], output_file: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª–∏—Å—Ç–∞–º–∏"""
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # –û—Å–Ω–æ–≤–Ω–∞—è —Å–≤–æ–¥–∫–∞
            summary_data = []
            for r in results:
                summary_data.append({
                    '–î–æ–º–µ–Ω': r['domain'],
                    'PBN Score': f"{r['pbn_score']:.1f}",
                    '–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è PBN': '‚úÖ' if r['metrics'].suitable_for_pbn else '‚ùå',
                    '–í–æ–∑—Ä–∞—Å—Ç (–¥–Ω–µ–π)': r['metrics'].domain_age or 'N/A',
                    '–°–ø–∞–º %': f"{r['metrics'].spam_ratio * 100:.1f}%",
                    '–°–Ω–∏–º–∫–æ–≤ –≤ Wayback': r['metrics'].total_snapshots,
                    '–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞': r['content'].primary_niche,
                    '–û—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫': r['content'].primary_language,
                    '–†–∏—Å–∫': f"{r['metrics'].risk_score:.1f}",
                    '–¶–µ–Ω–∞ Backorder': r['metrics'].backorder_price or 'N/A',
                    '–í–æ–∑—Ä–∞—Å—Ç (–ª–µ—Ç)': r['metrics'].backorder_age_years or 'N/A',
                    '–°—Å—ã–ª–∫–∏ (Backorder)': r['metrics'].backorder_links_count or 'N/A',
                    '–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –†–ö–ù': '–î–∞' if r['metrics'].backorder_rkn_block else '–ù–µ—Ç',
                    '–°—É–¥–µ–±–Ω—ã–π –¥–æ–º–µ–Ω': '–î–∞' if r['metrics'].backorder_judicial else '–ù–µ—Ç',
                    '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è': r['recommendations'][0] if r['recommendations'] else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'
                })

            df_summary = pd.DataFrame(summary_data)
            df_summary = df_summary.sort_values('PBN Score', ascending=False)
            df_summary.to_excel(writer, sheet_name='–°–≤–æ–¥–∫–∞', index=False)

            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            detailed_data = []
            for r in results:
                detailed_data.append({
                    '–î–æ–º–µ–Ω': r['domain'],
                    '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏': r['metrics'].registration_date.strftime('%Y-%m-%d') if r[
                        'metrics'].registration_date else 'N/A',
                    '–î–∞—Ç–∞ –∏—Å—Ç–µ—á–µ–Ω–∏—è': r['metrics'].expiry_date.strftime('%Y-%m-%d') if r[
                        'metrics'].expiry_date else 'N/A',
                    '–†–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä': r['metrics'].registrar or 'N/A',
                    '–ü–µ—Ä–≤—ã–π —Å–Ω–∏–º–æ–∫': r['metrics'].first_snapshot.strftime('%Y-%m-%d') if r[
                        'metrics'].first_snapshot else 'N/A',
                    '–ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–Ω–∏–º–æ–∫': r['metrics'].last_snapshot.strftime('%Y-%m-%d') if r[
                        'metrics'].last_snapshot else 'N/A',
                    '–ß–∞—Å—Ç–æ—Ç–∞ —Å–Ω–∏–º–∫–æ–≤/–º–µ—Å': f"{r['metrics'].snapshot_frequency:.2f}",
                    '–°–º–µ–Ω–∞ —Ç–µ–º–∞—Ç–∏–∫': len(r['metrics'].niche_changes),
                    '–°–º–µ–Ω–∞ —è–∑—ã–∫–æ–≤': len(r['metrics'].language_changes),
                    '–ò—Å—Ç–æ—Ä–∏—è CMS': ', '.join(r['metrics'].cms_history) if r['metrics'].cms_history else 'N/A',
                    '–¶–µ–Ω–∞ Backorder': r['metrics'].backorder_price or 'N/A',
                    '–í–æ–∑—Ä–∞—Å—Ç (–ª–µ—Ç)': r['metrics'].backorder_age_years or 'N/A',
                    '–°—Å—ã–ª–∫–∏ (Backorder)': r['metrics'].backorder_links_count or 'N/A',
                    '–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –†–ö–ù': '–î–∞' if r['metrics'].backorder_rkn_block else '–ù–µ—Ç',
                    '–°—É–¥–µ–±–Ω—ã–π –¥–æ–º–µ–Ω': '–î–∞' if r['metrics'].backorder_judicial else '–ù–µ—Ç',
                    '–°—Ä. –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞': f"{r['content'].avg_content_length:.0f}",
                    '–°—Ä. –≤–Ω—É—Ç—Ä. —Å—Å—ã–ª–æ–∫': f"{r['metrics'].internal_links_avg:.1f}",
                    '–°—Ä. –≤–Ω–µ—à–Ω. —Å—Å—ã–ª–æ–∫': f"{r['metrics'].external_links_avg:.1f}"
                })

            df_detailed = pd.DataFrame(detailed_data)
            df_detailed.to_excel(writer, sheet_name='–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑', index=False)

            # –ò—Å—Ç–æ—Ä–∏—è —Å–ø–∞–º–∞
            spam_data = []
            for r in results:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–∞–º-–ø–µ—Ä–∏–æ–¥–∞—Ö
                if 'spam_periods_with_keywords' in r and r['spam_periods_with_keywords']:
                    for start, end, keywords in r['spam_periods_with_keywords']:
                        spam_data.append({
                            '–î–æ–º–µ–Ω': r['domain'],
                            '–¢–∏–ø': '–°–ø–∞–º',
                            '–ù–∞—á–∞–ª–æ': start.strftime('%Y-%m-%d'),
                            '–ö–æ–Ω–µ—Ü': end.strftime('%Y-%m-%d'),
                            '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–¥–Ω–µ–π)': (end - start).days,
                            '–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ø–∞–º-—Å–ª–æ–≤–∞': ', '.join(keywords[:10]) if keywords else 'N/A',  # –ü–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤
                            '–í—Å–µ–≥–æ —Å–ø–∞–º-—Å–ª–æ–≤': len(keywords)
                        })
                if r['metrics'].clean_periods:
                    for start, end in r['metrics'].clean_periods:
                        spam_data.append({
                            '–î–æ–º–µ–Ω': r['domain'],
                            '–¢–∏–ø': '–ß–∏—Å—Ç—ã–π',
                            '–ù–∞—á–∞–ª–æ': start.strftime('%Y-%m-%d'),
                            '–ö–æ–Ω–µ—Ü': end.strftime('%Y-%m-%d'),
                            '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–¥–Ω–µ–π)': (end - start).days,
                            '–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ø–∞–º-—Å–ª–æ–≤–∞': '',
                            '–í—Å–µ–≥–æ —Å–ø–∞–º-—Å–ª–æ–≤': 0
                        })

            if spam_data:
                df_spam = pd.DataFrame(spam_data)
                df_spam = df_spam.sort_values(['–î–æ–º–µ–Ω', '–ù–∞—á–∞–ª–æ'])
                df_spam.to_excel(writer, sheet_name='–ò—Å—Ç–æ—Ä–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞', index=False)

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é —Å–Ω–∏–º–∫–æ–≤
            if any('content_history_detailed' in r for r in results):
                snapshot_data = []
                for r in results:
                    if 'content_history_detailed' in r:
                        for snapshot in r['content_history_detailed']:
                            snapshot_data.append({
                                '–î–æ–º–µ–Ω': r['domain'],
                                '–î–∞—Ç–∞ —Å–Ω–∏–º–∫–∞': snapshot['timestamp'].strftime('%Y-%m-%d %H:%M'),
                                'URL —Å–Ω–∏–º–∫–∞': snapshot['url'],
                                '–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ø–∞–º': '–î–∞' if snapshot['is_spam'] else '–ù–µ—Ç',
                                '–°–ø–∞–º-—Å–ª–æ–≤–∞': ', '.join(snapshot['spam_keywords'][:5]) if snapshot[
                                    'spam_keywords'] else '',
                                '–Ø–∑—ã–∫': snapshot['language'],
                                '–¢–µ–º–∞—Ç–∏–∫–∞': snapshot['niche'],
                                'CMS': snapshot['cms'],
                                '–†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞': snapshot['content_length']
                            })

                if snapshot_data:
                    df_snapshots = pd.DataFrame(snapshot_data)
                    df_snapshots.to_excel(writer, sheet_name='–î–µ—Ç–∞–ª–∏ —Å–Ω–∏–º–∫–æ–≤', index=False)

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations_data = []
            for r in results:
                for rec in r['recommendations']:
                    recommendations_data.append({
                        '–î–æ–º–µ–Ω': r['domain'],
                        'PBN Score': f"{r['pbn_score']:.1f}",
                        '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è': rec
                    })
                for warning in r.get('warnings', []):
                    recommendations_data.append({
                        '–î–æ–º–µ–Ω': r['domain'],
                        'PBN Score': f"{r['pbn_score']:.1f}",
                        '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è': f"‚ö†Ô∏è {warning}"
                    })

            if recommendations_data:
                df_rec = pd.DataFrame(recommendations_data)
                df_rec.to_excel(writer, sheet_name='–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', index=False)

            # –¢–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è PBN
            top_candidates = [r for r in results if r['pbn_score'] >= 70]
            if top_candidates:
                top_data = []
                for r in sorted(top_candidates, key=lambda x: x['pbn_score'], reverse=True)[:20]:
                    top_data.append({
                        '–î–æ–º–µ–Ω': r['domain'],
                        'PBN Score': f"{r['pbn_score']:.1f}",
                        '–í–æ–∑—Ä–∞—Å—Ç (–ª–µ—Ç)': f"{(r['metrics'].domain_age or 0) / 365:.1f}",
                        '–¢–µ–º–∞—Ç–∏–∫–∞': r['content'].primary_niche,
                        '–ü—Ä–∏—á–∏–Ω—ã –≤—ã–±–æ—Ä–∞': ' | '.join(r['recommendations'][:2])
                    })

                df_top = pd.DataFrame(top_data)
                df_top.to_excel(writer, sheet_name='–¢–û–ü –¥–ª—è PBN', index=False)

        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è SEO

class BacklinkChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ Wayback (–±–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è)"""

    @staticmethod
    async def check_domain_mentions(domain: str, session: aiohttp.ClientSession) -> Dict:
        """–ò—â–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–æ–º–µ–Ω–∞ –≤ Wayback"""
        # –ò—â–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å—Å—ã–ª–∞–ª–∏—Å—å –Ω–∞ –¥–æ–º–µ–Ω
        api_url = f"https://web.archive.org/cdx/search/cdx?url=*.{domain}&output=json&limit=100"

        try:
            async with session.get(api_url) as response:
                if response.status == 200:
                    data = await response.json()
                    if len(data) > 1:
                        return {
                            'mentions_count': len(data) - 1,
                            'unique_domains': len(set(urlparse(r[1]).netloc for r in data[1:]))
                        }
        except:
            pass

        return {'mentions_count': 0, 'unique_domains': 0}


class DomainMetricsExporter:
    """–≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""

    @staticmethod
    def export_for_ahrefs_import(results: List[Dict], filename: str = "domains_for_ahrefs.txt"):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–æ–º–µ–Ω—ã –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ Ahrefs"""
        domains = [r['domain'] for r in results if r['pbn_score'] >= 60]

        with open(filename, 'w') as f:
            f.write('\n'.join(domains))

        logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(domains)} –¥–æ–º–µ–Ω–æ–≤ –≤ {filename}")

    @staticmethod
    def export_monitoring_list(results: List[Dict], filename: str = "domains_monitoring.json"):
        """–°–æ–∑–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–æ–º–µ–Ω–æ–≤"""
        monitoring = []

        for r in results:
            if r['pbn_score'] >= 50:
                monitoring.append({
                    'domain': r['domain'],
                    'score': r['pbn_score'],
                    'expiry_date': r['metrics'].expiry_date.isoformat() if r['metrics'].expiry_date else None,
                    'check_priority': 'high' if r['pbn_score'] >= 80 else 'medium'
                })

        with open(filename, 'w') as f:
            json.dump(monitoring, f, indent=2)

        logger.info(f"–°–æ–∑–¥–∞–Ω —Å–ø–∏—Å–æ–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {filename}")


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–º–µ–Ω—ã
    with open('outgoing_domains1.txt', 'r') as f:
        domains = [line.strip() for line in f if line.strip()]

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    domains = list(set(domains))
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(domains)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
    analyzer = PBNBulkAnalyzer(max_concurrent=5)
    results = await analyzer.analyze_domains(domains, output_file="pbn_analysis.xlsx")

    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    DomainMetricsExporter.export_for_ahrefs_import(results)
    DomainMetricsExporter.export_monitoring_list(results)

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    suitable_count = sum(1 for r in results if r['metrics'].suitable_for_pbn)
    top_count = sum(1 for r in results if r['pbn_score'] >= 80)

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê:")
    print(f"–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(results)} –¥–æ–º–µ–Ω–æ–≤")
    print(f"–ü–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è PBN: {suitable_count} ({suitable_count / len(results) * 100:.1f}%)")
    print(f"–û—Ç–ª–∏—á–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (80+): {top_count}")
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ pbn_analysis.xlsx")


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã:
    # pip install aiohttp pandas beautifulsoup4 lxml python-whois dnspython numpy openpyxl pyahocorasick

    asyncio.run(main())